{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 2. Classifiers:\n",
      "\n",
      " 2.1 NB\n",
      "a) Suggested parameterization: MultinomialNB with Undersampling\n",
      "b) Confusion matrix: \n",
      "[[1247 1045    4    0  139   13  299]\n",
      " [ 564 1511   47    9  483  116   17]\n",
      " [   0    7 1145  579   87  929    0]\n",
      " [   0    0  290 2151    0  306    0]\n",
      " [ 318  639  235    0 1481   74    0]\n",
      " [  38  135  370  394  254 1556    0]\n",
      " [ 166  326    9    0   24    0 2222]]\n",
      "\n",
      " 2.2 kNN\n",
      "a) Suggested parameterization: n_neighbors=1, metric='manhattan' with Undersampling\n",
      "b) Confusion matrix: \n",
      "[[2146  410    0    0   53    8  130]\n",
      " [ 429 1939   54    4  226   66   29]\n",
      " [   0   33 2312  137   25  240    0]\n",
      " [   0    0   58 2636    0   53    0]\n",
      " [  10   60   18    0 2641   17    1]\n",
      " [   2   14  162   72   30 2467    0]\n",
      " [  45    8    0    0    1    0 2693]]\n",
      "\n",
      " 2.2 DT\n",
      "a) Suggested parameterization: max_depth=13, min_samples_split=0.0002, min_samples_leaf=0.00005, criterion='entropy' with Undersampling\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-6e77b70364ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1090\u001b[0m     \u001b[0;34m'''C: output results'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1091\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1092\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-6e77b70364ce>\u001b[0m in \u001b[0;36mreport\u001b[0;34m(source, dataframe, task)\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mclassification_pd_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msource\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"CT\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1060\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mclassification_ct_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1061\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-6e77b70364ce>\u001b[0m in \u001b[0;36mclassification_ct_report\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    978\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 980\u001b[0;31m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcross_val_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    981\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    982\u001b[0m     \u001b[0macuDT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mcross_val_predict\u001b[0;34m(estimator, X, y, groups, cv, n_jobs, verbose, fit_params, pre_dispatch, method)\u001b[0m\n\u001b[1;32m    778\u001b[0m     prediction_blocks = parallel(delayed(_fit_and_predict)(\n\u001b[1;32m    779\u001b[0m         clone(estimator), X, y, train, test, verbose, fit_params, method)\n\u001b[0;32m--> 780\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m     \u001b[0;31m# Concatenate the predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_predict\u001b[0;34m(estimator, X, y, train, test, verbose, fit_params, method)\u001b[0m\n\u001b[1;32m    865\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 867\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    868\u001b[0m     \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    869\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    814\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    815\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 816\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    817\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    378\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import sys, pandas as pd\n",
    "from preprocessing_pd import preprocessing_pd_report\n",
    "from preprocessing_ct import preprocessing_ct_report\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Normalizer\n",
    "import matplotlib.pyplot as plt\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "\n",
    "from sklearn.metrics import silhouette_score, adjusted_rand_score, confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "#import itertools\n",
    "\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import LeavePOut\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import xgboost as xgb\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unsupervised_pd_report(data):\n",
    "    \n",
    "    data = data.sort_values('id', ascending=True)\n",
    "\n",
    "    #Fazer a média das 3 medições com o mesmo id\n",
    "    data = data.groupby('id').mean().reset_index()\n",
    "    \n",
    "    # Normalization\n",
    "    transf = MinMaxScaler().fit(data)\n",
    "    data = pd.DataFrame(transf.transform(data), columns= data.columns)\n",
    "    \n",
    "    \n",
    "    X = data.drop(columns=['class', 'id'])\n",
    "    y = data['class'].values\n",
    "\n",
    "    from sklearn.decomposition import PCA\n",
    "    X = PCA(n_components=2, random_state=1).fit_transform(X)\n",
    "\n",
    "    from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "    #X = SelectKBest(f_classif, k=2).fit_transform(X, y)\n",
    "\n",
    "    plt.subplot(111)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.show()\n",
    "    n_of_cluster = []\n",
    "    inertia_values = []\n",
    "    silhouette_values = []\n",
    "    rand_index_values = []\n",
    "\n",
    "    for i in range(2, 10):\n",
    "        kmeans_model = cluster.KMeans(n_clusters=i, random_state=1).fit(X)\n",
    "        y_pred = kmeans_model.labels_\n",
    "        n_of_cluster.append(i) \n",
    "        inertia_values.append(kmeans_model.inertia_)\n",
    "        silhouette_values.append(silhouette_score(X, y_pred))\n",
    "        rand_index_values.append(adjusted_rand_score(y, y_pred))\n",
    "\n",
    "        # Inertia, diferença entre valores dentro do mesmo cluster, quão coerentes os clusters são internamente, o objetivo é minimizar este valor\n",
    "        plt.plot(n_of_cluster, inertia_values)\n",
    "        plt.xlabel(\"Nr of clusters\")\n",
    "        plt.ylabel(\"Inertia\")\n",
    "        plt.show()\n",
    "\n",
    "        # Silhouette, quão bem classificado está um sample no seu cluster, quanto mais alto for o valor melhor definido está o cluster\n",
    "        plt.plot(n_of_cluster, silhouette_values)\n",
    "        plt.xlabel(\"Nr of clusters\")\n",
    "        plt.ylabel(\"Silhoutte score\")\n",
    "        plt.show()\n",
    "\n",
    "        # Ajusted Rand Indexes, é a similariedade entre dois clusters, usamos para comparar o predicted com o verdadeiro\n",
    "        # requer saber as classes verdadeiras, \n",
    "        # o valor 1.0 que é o mais alto é o perfeito significa classificação de labels perfeita\n",
    "        # distribuição uniforme (random) de labels tem um valor de 0.0\n",
    "        plt.plot(n_of_cluster, rand_index_values)\n",
    "        plt.xlabel(\"Nr of clusters\")\n",
    "        plt.ylabel(\"Rand index score\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(n_of_cluster, silhouette_values, label=\"Silhoutte score\")\n",
    "        plt.plot(n_of_cluster, rand_index_values, label=\"Rand Index score\")\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xlabel(\"Nr of clusters\")\n",
    "        plt.show()\n",
    "    \n",
    "    algorithms = {}\n",
    "    n_clusters = 2\n",
    "\n",
    "    # 1b Parameterize clustering algorithms\n",
    "    algorithms['K Means'] = cluster.KMeans(n_clusters=n_clusters, random_state=1)\n",
    "    algorithms['Ward Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    algorithms['Average Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "\n",
    "    # 3 Run clustering algorithm and store predictions\n",
    "    predictions = {}\n",
    "    efficiency = {}\n",
    "    for idx, name in enumerate(algorithms):\n",
    "        clustering = algorithms[name]\n",
    "        t0 = time.time()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the connectivity matrix is [0-9]{1,2}\" +\n",
    "                    \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            clustering.fit(X)\n",
    "        efficiency[name]= time.time()-t0\n",
    "        if hasattr(clustering, 'labels_'): predictions[name] = clustering.labels_.astype(np.int)\n",
    "        else: predictions[name] = clustering.predict(X)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(11, 8))\n",
    "        plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
    "        color_array = ['#377eb8','#ff7f00','#4daf4a','#f781bf','#a65628','#984ea3','#999999','#e41a1c','#dede00']\n",
    "        plot_num = 1\n",
    "\n",
    "        for idx, name in enumerate(predictions):\n",
    "            y_predi = predictions[name]\n",
    "            plt.subplot(2, 3, plot_num)\n",
    "            plt.tight_layout()\n",
    "            plt.title(name+\" - \"+str(n_clusters)+\" clusters\", size=15)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            colors = np.array(list(islice(cycle(color_array),int(max(y_predi) + 1))))\n",
    "            colors = np.append(colors, [\"#000000\"]) #black color for outliers (if any)\n",
    "            plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_predi])\n",
    "            silh = str('%.3f'%(silhouette_score(X, y_predi)))\n",
    "            ari = str('%.3f'%(adjusted_rand_score(y, y_predi)))\n",
    "            plt.text(.99, .01, 'ARI '+ari+', Silhouette '+silh, transform=plt.gca().transAxes,size=10,horizontalalignment='right')\n",
    "            plot_num += 1\n",
    "\n",
    "        plt.subplot(2, 3, plot_num)\n",
    "        plt.title(\"True\", size=18)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, c=y)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    algorithms = {}\n",
    "    n_clusters = 3\n",
    "\n",
    "    # 1b Parameterize clustering algorithms\n",
    "    algorithms['K Means'] = cluster.KMeans(n_clusters=n_clusters, random_state=1)\n",
    "    algorithms['Ward Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    algorithms['Average Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "\n",
    "    # 3 Run clustering algorithm and store predictions\n",
    "    predictions = {}\n",
    "    efficiency = {}\n",
    "    for idx, name in enumerate(algorithms):\n",
    "        clustering = algorithms[name]\n",
    "        t0 = time.time()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the connectivity matrix is [0-9]{1,2}\" +\n",
    "                    \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            clustering.fit(X)\n",
    "        efficiency[name]= time.time()-t0\n",
    "        if hasattr(clustering, 'labels_'): predictions[name] = clustering.labels_.astype(np.int)\n",
    "        else: predictions[name] = clustering.predict(X)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(11, 8))\n",
    "        plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
    "        color_array = ['#377eb8','#ff7f00','#4daf4a','#f781bf','#a65628','#984ea3','#999999','#e41a1c','#dede00']\n",
    "        plot_num = 1\n",
    "\n",
    "        for idx, name in enumerate(predictions):\n",
    "            y_predi = predictions[name]\n",
    "            plt.subplot(2, 3, plot_num)\n",
    "            plt.tight_layout()\n",
    "            plt.title(name+\" - \"+str(n_clusters)+\" clusters\", size=15)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            colors = np.array(list(islice(cycle(color_array),int(max(y_predi) + 1))))\n",
    "            colors = np.append(colors, [\"#000000\"]) #black color for outliers (if any)\n",
    "            plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_predi])\n",
    "            silh = str('%.3f'%(silhouette_score(X, y_predi)))\n",
    "            ari = str('%.3f'%(adjusted_rand_score(y, y_predi)))\n",
    "            plt.text(.99, .01, 'ARI '+ari+', Silhouette '+silh, transform=plt.gca().transAxes,size=10,horizontalalignment='right')\n",
    "            plot_num += 1\n",
    "\n",
    "        plt.subplot(2, 3, plot_num)\n",
    "        plt.title(\"True\", size=18)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, c=y)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    algorithms = {}\n",
    "    n_clusters = 4\n",
    "\n",
    "    # 1b Parameterize clustering algorithms\n",
    "    algorithms['K Means'] = cluster.KMeans(n_clusters=n_clusters, random_state=1)\n",
    "    algorithms['Ward Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    algorithms['Average Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "\n",
    "    # 3 Run clustering algorithm and store predictions\n",
    "    predictions = {}\n",
    "    efficiency = {}\n",
    "    for idx, name in enumerate(algorithms):\n",
    "        clustering = algorithms[name]\n",
    "        t0 = time.time()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the connectivity matrix is [0-9]{1,2}\" +\n",
    "                    \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            clustering.fit(X)\n",
    "        efficiency[name]= time.time()-t0\n",
    "        if hasattr(clustering, 'labels_'): predictions[name] = clustering.labels_.astype(np.int)\n",
    "        else: predictions[name] = clustering.predict(X)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(11, 8))\n",
    "        plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
    "        color_array = ['#377eb8','#ff7f00','#4daf4a','#f781bf','#a65628','#984ea3','#999999','#e41a1c','#dede00']\n",
    "        plot_num = 1\n",
    "\n",
    "        for idx, name in enumerate(predictions):\n",
    "            y_predi = predictions[name]\n",
    "            plt.subplot(2, 3, plot_num)\n",
    "            plt.tight_layout()\n",
    "            plt.title(name+\" - \"+str(n_clusters)+\" clusters\", size=15)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            colors = np.array(list(islice(cycle(color_array),int(max(y_predi) + 1))))\n",
    "            colors = np.append(colors, [\"#000000\"]) #black color for outliers (if any)\n",
    "            plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_predi])\n",
    "            silh = str('%.3f'%(silhouette_score(X, y_predi)))\n",
    "            ari = str('%.3f'%(adjusted_rand_score(y, y_predi)))\n",
    "            plt.text(.99, .01, 'ARI '+ari+', Silhouette '+silh, transform=plt.gca().transAxes,size=10,horizontalalignment='right')\n",
    "            plot_num += 1\n",
    "\n",
    "        plt.subplot(2, 3, plot_num)\n",
    "        plt.title(\"True\", size=18)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, c=y)\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def unsupervised_ct_report(data):\n",
    "    X = data.drop(columns=['Cover_Type'])\n",
    "    y = data['Cover_Type'].values\n",
    "    print(X.shape)\n",
    "\n",
    "    # não há missing values\n",
    "    from sklearn.impute import SimpleImputer\n",
    "\n",
    "    try:\n",
    "        imp_nr = SimpleImputer(strategy='mean', missing_values=np.nan, copy=True)\n",
    "        imp_sb = SimpleImputer(strategy='most_frequent', missing_values='', copy=True)\n",
    "        df_nr = pd.DataFrame(imp_nr.fit_transform(cols_nr), columns=cols_nr.columns)\n",
    "        df_sb = pd.DataFrame(imp_sb.fit_transform(cols_sb), columns=cols_sb.columns)\n",
    "    except:\n",
    "        print(\"No missing values\") \n",
    "\n",
    "    #Não se faz dummify, pois não existem features do tipo category \n",
    "    cols_sb = data.select_dtypes(include='category')\n",
    "    if cols_sb.empty:\n",
    "        print('No category type columns!') #shows no categoriy types in the dataset\n",
    "    else:\n",
    "        print('Exists category type columns!')\n",
    "\n",
    "    # Hold-Out \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.995, random_state=13)\n",
    "    print(X_train.shape)\n",
    "\n",
    "    # normalize\n",
    "    from sklearn.preprocessing import Normalizer\n",
    "\n",
    "    transf = StandardScaler().fit(X_train)\n",
    "    data = pd.DataFrame(transf.transform(X_train))\n",
    "    data.describe(include='all')\n",
    "    \n",
    "    X = data\n",
    "    y = y_train\n",
    "\n",
    "    #from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=2, random_state=1).fit(X)\n",
    "    X = pca.transform(X)\n",
    "\n",
    "    plt.subplot(111)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "    plt.show()\n",
    "    \n",
    "    n_of_cluster = []\n",
    "    inertia_values = []\n",
    "    silhouette_values = []\n",
    "    rand_index_values = []\n",
    "\n",
    "    for i in range(2, 12):\n",
    "        kmeans_model = cluster.KMeans(n_clusters=i, random_state=1).fit(X)\n",
    "        y_pred = kmeans_model.labels_\n",
    "        n_of_cluster.append(i) \n",
    "        inertia_values.append(kmeans_model.inertia_)\n",
    "        silhouette_values.append(silhouette_score(X, y_pred))\n",
    "        rand_index_values.append(adjusted_rand_score(y, y_pred))\n",
    "\n",
    "    # Inertia, diferença entre valores dentro do mesmo cluster, quão coerentes os clusters são internamente, o objetivo é minimizar este valor\n",
    "    plt.plot(n_of_cluster, inertia_values)\n",
    "    plt.xlabel(\"Nr of clusters\")\n",
    "    plt.ylabel(\"Inertia\")\n",
    "    plt.show()\n",
    "\n",
    "    # Silhouette, quão bem classificado está um sample no seu cluster, quanto mais alto for o valor melhor definido está o cluster\n",
    "    plt.plot(n_of_cluster, silhouette_values)\n",
    "    plt.xlabel(\"Nr of clusters\")\n",
    "    plt.ylabel(\"Silhoutte score\")\n",
    "    plt.show()\n",
    "\n",
    "    # Ajusted Rand Indexes, é a similariedade entre dois clusters, usamos para comparar o predicted com o verdadeiro\n",
    "    # requer saber as classes verdadeiras, \n",
    "    # o valor 1.0 que é o mais alto é o perfeito significa classificação de labels perfeita\n",
    "    # distribuição uniforme (random) de labels tem um valor de 0.0\n",
    "    plt.plot(n_of_cluster, rand_index_values)\n",
    "    plt.xlabel(\"Nr of clusters\")\n",
    "    plt.ylabel(\"Rand index score\")\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(n_of_cluster, silhouette_values, label=\"Silhoutte score\")\n",
    "    plt.plot(n_of_cluster, rand_index_values, label=\"Rand Index score\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel(\"Nr of clusters\")\n",
    "    plt.show()\n",
    "    \n",
    "    algorithms = {}\n",
    "    n_clusters = 2\n",
    "\n",
    "    # 1b Parameterize clustering algorithms\n",
    "    algorithms['K Means'] = cluster.KMeans(n_clusters=n_clusters, random_state=1)\n",
    "    algorithms['Ward Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    algorithms['Average Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "\n",
    "    # 3 Run clustering algorithm and store predictions\n",
    "    predictions = {}\n",
    "    efficiency = {}\n",
    "    for idx, name in enumerate(algorithms):\n",
    "        clustering = algorithms[name]\n",
    "        t0 = time.time()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the connectivity matrix is [0-9]{1,2}\" +\n",
    "                    \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            clustering.fit(X)\n",
    "        efficiency[name]= time.time()-t0\n",
    "        if hasattr(clustering, 'labels_'): predictions[name] = clustering.labels_.astype(np.int)\n",
    "        else: predictions[name] = clustering.predict(X)\n",
    "\n",
    "    plt.figure(figsize=(11, 8))\n",
    "    plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
    "    color_array = ['#377eb8','#ff7f00','#4daf4a','#f781bf','#a65628','#984ea3','#999999','#e41a1c','#dede00']\n",
    "    plot_num = 1\n",
    "\n",
    "    for idx, name in enumerate(predictions):\n",
    "        y_predi = predictions[name]\n",
    "        plt.subplot(2, 3, plot_num)\n",
    "        plt.tight_layout()\n",
    "        plt.title(name, size=18)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        colors = np.array(list(islice(cycle(color_array),int(max(y_predi) + 1))))\n",
    "        colors = np.append(colors, [\"#000000\"]) #black color for outliers (if any)\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_predi])\n",
    "        silh = str('%.3f'%(silhouette_score(X, y_predi)))\n",
    "        ari = str('%.3f'%(adjusted_rand_score(y, y_predi)))\n",
    "        plt.text(.99, .01, 'ARI '+ari+', Silhouette '+silh, transform=plt.gca().transAxes,size=10,horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "    plt.subplot(2, 3, plot_num)\n",
    "    plt.title(\"True\", size=18)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=10, c=y)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    algorithms = {}\n",
    "    n_clusters = 3\n",
    "\n",
    "    # 1b Parameterize clustering algorithms\n",
    "    algorithms['K Means'] = cluster.KMeans(n_clusters=n_clusters, random_state=1)\n",
    "    algorithms['Ward Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    algorithms['Average Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "\n",
    "    # 3 Run clustering algorithm and store predictions\n",
    "    predictions = {}\n",
    "    efficiency = {}\n",
    "    for idx, name in enumerate(algorithms):\n",
    "        clustering = algorithms[name]\n",
    "        t0 = time.time()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the connectivity matrix is [0-9]{1,2}\" +\n",
    "                    \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            clustering.fit(X)\n",
    "        efficiency[name]= time.time()-t0\n",
    "        if hasattr(clustering, 'labels_'): predictions[name] = clustering.labels_.astype(np.int)\n",
    "        else: predictions[name] = clustering.predict(X)\n",
    "\n",
    "\n",
    "        plt.figure(figsize=(11, 8))\n",
    "        plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
    "        color_array = ['#377eb8','#ff7f00','#4daf4a','#f781bf','#a65628','#984ea3','#999999','#e41a1c','#dede00']\n",
    "        plot_num = 1\n",
    "\n",
    "        for idx, name in enumerate(predictions):\n",
    "            y_predi = predictions[name]\n",
    "            plt.subplot(2, 3, plot_num)\n",
    "            plt.tight_layout()\n",
    "            plt.title(name, size=18)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            colors = np.array(list(islice(cycle(color_array),int(max(y_predi) + 1))))\n",
    "            colors = np.append(colors, [\"#000000\"]) #black color for outliers (if any)\n",
    "            plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_predi])\n",
    "            silh = str('%.3f'%(silhouette_score(X, y_predi)))\n",
    "            ari = str('%.3f'%(adjusted_rand_score(y, y_predi)))\n",
    "            plt.text(.99, .01, 'ARI '+ari+', Silhouette '+silh, transform=plt.gca().transAxes,size=10,horizontalalignment='right')\n",
    "            plot_num += 1\n",
    "\n",
    "        plt.subplot(2, 3, plot_num)\n",
    "        plt.title(\"True\", size=18)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, c=y)\n",
    "\n",
    "        plt.show()\n",
    "    \n",
    "    algorithms = {}\n",
    "    n_clusters = 4\n",
    "\n",
    "    # 1b Parameterize clustering algorithms\n",
    "    algorithms['K Means'] = cluster.KMeans(n_clusters=n_clusters, random_state=1)\n",
    "    algorithms['Ward Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    algorithms['Average Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "\n",
    "    # 3 Run clustering algorithm and store predictions\n",
    "    predictions = {}\n",
    "    efficiency = {}\n",
    "    for idx, name in enumerate(algorithms):\n",
    "        clustering = algorithms[name]\n",
    "        t0 = time.time()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the connectivity matrix is [0-9]{1,2}\" +\n",
    "                    \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            clustering.fit(X)\n",
    "        efficiency[name]= time.time()-t0\n",
    "        if hasattr(clustering, 'labels_'): predictions[name] = clustering.labels_.astype(np.int)\n",
    "        else: predictions[name] = clustering.predict(X)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(11, 8))\n",
    "    plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
    "    color_array = ['#377eb8','#ff7f00','#4daf4a','#f781bf','#a65628','#984ea3','#999999','#e41a1c','#dede00']\n",
    "    plot_num = 1\n",
    "\n",
    "    for idx, name in enumerate(predictions):\n",
    "        y_predi = predictions[name]\n",
    "        plt.subplot(2, 3, plot_num)\n",
    "        plt.tight_layout()\n",
    "        plt.title(name, size=18)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        colors = np.array(list(islice(cycle(color_array),int(max(y_predi) + 1))))\n",
    "        colors = np.append(colors, [\"#000000\"]) #black color for outliers (if any)\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_predi])\n",
    "        silh = str('%.3f'%(silhouette_score(X, y_predi)))\n",
    "        ari = str('%.3f'%(adjusted_rand_score(y, y_predi)))\n",
    "        plt.text(.99, .01, 'ARI '+ari+', Silhouette '+silh, transform=plt.gca().transAxes,size=10,horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "    plt.subplot(2, 3, plot_num)\n",
    "    plt.title(\"True\", size=18)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=10, c=y)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    algorithms = {}\n",
    "    n_clusters = 5\n",
    "\n",
    "    # 1b Parameterize clustering algorithms\n",
    "    algorithms['K Means'] = cluster.KMeans(n_clusters=n_clusters, random_state=1)\n",
    "    algorithms['Ward Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='ward')\n",
    "    algorithms['Average Linkage'] = cluster.AgglomerativeClustering(n_clusters=n_clusters, linkage='average')\n",
    "\n",
    "    # 3 Run clustering algorithm and store predictions\n",
    "    predictions = {}\n",
    "    efficiency = {}\n",
    "    for idx, name in enumerate(algorithms):\n",
    "        clustering = algorithms[name]\n",
    "        t0 = time.time()\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.filterwarnings(\n",
    "                \"ignore\",\n",
    "                message=\"the number of connected components of the connectivity matrix is [0-9]{1,2}\" +\n",
    "                    \" > 1. Completing it to avoid stopping the tree early.\",\n",
    "                category=UserWarning)\n",
    "            clustering.fit(X)\n",
    "        efficiency[name]= time.time()-t0\n",
    "        if hasattr(clustering, 'labels_'): predictions[name] = clustering.labels_.astype(np.int)\n",
    "        else: predictions[name] = clustering.predict(X)\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(11, 8))\n",
    "    plt.subplots_adjust(left=.02, right=.98, bottom=.001, top=.96, wspace=.05, hspace=.01)\n",
    "    color_array = ['#377eb8','#ff7f00','#4daf4a','#f781bf','#a65628','#984ea3','#999999','#e41a1c','#dede00']\n",
    "    plot_num = 1\n",
    "\n",
    "    for idx, name in enumerate(predictions):\n",
    "        y_predi = predictions[name]\n",
    "        plt.subplot(2, 3, plot_num)\n",
    "        plt.tight_layout()\n",
    "        plt.title(name, size=18)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        colors = np.array(list(islice(cycle(color_array),int(max(y_predi) + 1))))\n",
    "        colors = np.append(colors, [\"#000000\"]) #black color for outliers (if any)\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=10, color=colors[y_predi])\n",
    "        silh = str('%.3f'%(silhouette_score(X, y_predi)))\n",
    "        ari = str('%.3f'%(adjusted_rand_score(y, y_predi)))\n",
    "        plt.text(.99, .01, 'ARI '+ari+', Silhouette '+silh, transform=plt.gca().transAxes,size=10,horizontalalignment='right')\n",
    "        plot_num += 1\n",
    "\n",
    "    plt.subplot(2, 3, plot_num)\n",
    "    plt.title(\"True\", size=18)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=10, c=y)\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def classification_pd_report(data):\n",
    "    #using a 10-fold cross validation\n",
    "    #Fazer a média das 3 medições\n",
    "    data = data.groupby('id').mean().reset_index()\n",
    "\n",
    "    y = data.pop('class').values\n",
    "    df2 = pd.DataFrame(y, columns=['class'])\n",
    "    transf = Normalizer().fit(data)\n",
    "    norm_data = pd.DataFrame(transf.transform(data, copy=True), columns= data.columns)\n",
    "    norm_data = pd.concat([norm_data, df2], axis=1)\n",
    "    data = norm_data\n",
    "    \n",
    "    \n",
    "\n",
    "    init_n_rows = str(data.shape[0])\n",
    "    data = data.sort_values('id', ascending=True)\n",
    "    data = data.groupby('id').mean().reset_index()\n",
    "    final_n_rows = str(data.shape[0])\n",
    "\n",
    "    unbal = data\n",
    "    \n",
    "    unbal = data\n",
    "    target_count = unbal['class'].value_counts()\n",
    "    \n",
    "    min_class = target_count.idxmin()\n",
    "    ind_min_class = target_count.index.get_loc(min_class)\n",
    "    print(min_class)\n",
    "\n",
    "    df_class_min = unbal[unbal['class'] == min_class]\n",
    "    df_class_max = unbal[unbal['class'] != min_class]\n",
    "    \n",
    "    RANDOM_STATE = 42\n",
    "    target_values_0 = target_count.values[ind_min_class]\n",
    "    target_values_1 = target_count.values[1-ind_min_class]\n",
    "    values = {'data': [target_values_0, target_values_1]}\n",
    "\n",
    "    df_under = df_class_max.sample(len(df_class_min))\n",
    "    values['UnderSample'] = [target_count.values[ind_min_class], len(df_under)]\n",
    "\n",
    "    df_over = df_class_min.sample(len(df_class_max), replace=True)\n",
    "    values['OverSample'] = [len(df_over), target_count.values[1-ind_min_class]]\n",
    "\n",
    "    smote = SMOTE(ratio='minority', random_state=RANDOM_STATE)\n",
    "\n",
    "    y = unbal.pop('class').values\n",
    "    X = unbal.values\n",
    "    smote_x, smote_y = smote.fit_sample(X, y)\n",
    "    smote_target_count = pd.Series(smote_y).value_counts()\n",
    "\n",
    "    df_SMOTE = pd.DataFrame(smote_x)\n",
    "    df_SMOTE.columns = unbal.columns\n",
    "    df_SMOTE['class'] = smote_y\n",
    "\n",
    "    values['SMOTE'] = [smote_target_count.values[ind_min_class], smote_target_count.values[1-ind_min_class]]\n",
    "    \n",
    "    data = df_SMOTE\n",
    "\n",
    "    data.to_csv(r'data.csv', index = False)\n",
    "\n",
    "    #feature selection\n",
    "\n",
    "    y = data.pop('class').values\n",
    "    X = data.values\n",
    "    \n",
    "    X_norm = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "    X_norm = pd.DataFrame(X_norm, columns=data.columns)\n",
    "    kbest = SelectKBest(chi2, k=100)\n",
    "    X_new = kbest.fit_transform(X_norm, y)\n",
    "\n",
    "    column_names = data.columns[kbest.get_support()]\n",
    "    X_new = pd.DataFrame(X_norm, columns=column_names)\n",
    "\n",
    "    df2 = pd.DataFrame(y, columns=['class'])\n",
    "    data = pd.concat([X_new, df2], axis=1)\n",
    "\n",
    "    print(\"2. Classifiers:\\n \")\n",
    "\n",
    "    labels = pd.unique(y)\n",
    "\n",
    "   \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None, stratify=y)\n",
    "    \n",
    "    cv = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "    \n",
    "    y = data.pop('class').values\n",
    "    X = data.values\n",
    "\n",
    "    print(\"2.1 NB\\n\")\n",
    "    print(\"a) Suggested parameterization: MultinomialNB with SMOTE and KBest feature selection\")\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    scoresNB = []\n",
    "    y_predict_total =[]\n",
    "    y_test_total =[]\n",
    "    tn, fp, fn, tp = 0,0,0,0\n",
    "    #print(\"Initial shape: \"+str(X.shape))\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    \n",
    "    acuNB = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    #acuNB = round((tp+tn)/(tp+tn+fp+fn),2)\n",
    "    specificityNB = round(tn / (tn+fp),2)\n",
    "    sensitivityNB = round(tp / (tp + fn),2)\n",
    "    precisionNB = round(metrics.precision_score(y, y_pred),2)\n",
    "    f1NB = round(metrics.f1_score(y, y_pred),2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #kfold = model_selection.KFold(n_splits=10, random_state=100)\n",
    "    #model_kfold = MultinomialNB()\n",
    "    #results_kfold = model_selection.cross_val_score(model_kfold, X, y, cv=kfold)\n",
    "    #print(results_kfold)\n",
    "    \n",
    "    \n",
    "    #for train_index, test_index in cv.split(X):\n",
    "        \n",
    "        #X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n",
    "        \n",
    "        \n",
    "        \n",
    "        #model.fit(X_train, y_train)\n",
    "\n",
    "        #y_predict = gaussNB.predict(X_test)\n",
    "\n",
    "        #y_predict_total.extend(y_predict)\n",
    "        #y_test_total.extend(y_train)\n",
    "\n",
    "        #scoresNB.append(model.score(X_test, y_test))\n",
    "        \n",
    "    \n",
    "    \n",
    "    #print(np.mean(scoresNB))\n",
    "\n",
    "\n",
    "    #y_predict = model.predict(X_test)\n",
    "\n",
    "    #acu = accuracy_score(y_test, y_predict)\n",
    "    \n",
    "    #print(\"Accuracy:\", round(acu,2))\n",
    "    #tn, fp, fn, tp = confusion_matrix(y_test, y_predict).ravel()\n",
    "    \n",
    "    #print(\"Specificity: \", round(specificity,2))\n",
    "    \n",
    "    #print(\"Sensitivity: \", round(sensitivity,2))\n",
    "    \n",
    "    print(\"b) Confusion matrix: \")\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(conf_mat)\n",
    "    \n",
    "    print(\"2.2 kNN\\n\")\n",
    "    print(\"a) Suggested parameterization: n_neighbors= 49, metric= euclidean, with SMOTE and KBest feature selection\")\n",
    "\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=49, metric='euclidean')\n",
    "    scoresNB = []\n",
    "    y_predict_total =[]\n",
    "    y_test_total =[]\n",
    "    tn, fp, fn, tp = 0,0,0,0\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    \n",
    "    acukNN = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    #acuNB = round((tp+tn)/(tp+tn+fp+fn),2)\n",
    "    specificitykNN = round(tn / (tn+fp),2)\n",
    "    sensitivitykNN = round(tp / (tp + fn),2)\n",
    "    precisionkNN = round(metrics.precision_score(y, y_pred),2)\n",
    "    f1kNN = round(metrics.f1_score(y, y_pred),2)\n",
    "\n",
    "\n",
    "    print(\"b) Confusion matrix: \")\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(conf_mat)\n",
    "\n",
    "          \n",
    "    print(\"2.3 DT\\n\")\n",
    "    print(\"a) Suggested parameterization: max_depth=5, min_samples_split=0.1, min_samples_leaf=0.04, criterion=entropy with SMOTE and KBest feature selection\")\n",
    "\n",
    "\n",
    "    model = DecisionTreeClassifier(max_depth=5, min_samples_split=0.1, min_samples_leaf=0.04, criterion=\"entropy\", random_state=1)\n",
    "    scoresNB = []\n",
    "    y_predict_total =[]\n",
    "    y_test_total =[]\n",
    "    tn, fp, fn, tp = 0,0,0,0\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    \n",
    "    acuDT = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    #acuNB = round((tp+tn)/(tp+tn+fp+fn),2)\n",
    "    specificityDT = round(tn / (tn+fp),2)\n",
    "    sensitivityDT = round(tp / (tp + fn),2)\n",
    "    precisionDT= round(metrics.precision_score(y, y_pred),2)\n",
    "    f1DT = round(metrics.f1_score(y, y_pred),2)\n",
    "\n",
    "\n",
    "    print(\"b) Confusion matrix: \")\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(conf_mat)\n",
    "    \n",
    "    \n",
    "    print(\"2.5 RF\\n\")\n",
    "    print(\"a) Suggested parameterization: n_estimators=50, max_depth=25, max_features='log2' with SMOTE and KBest feature selection\")\n",
    "\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=50, max_depth=25, max_features='log2')\n",
    "\n",
    "    scoresNB = []\n",
    "    y_predict_total =[]\n",
    "    y_test_total =[]\n",
    "    tn, fp, fn, tp = 0,0,0,0\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    \n",
    "    acuRF = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    #acuNB = round((tp+tn)/(tp+tn+fp+fn),2)\n",
    "    specificityRF = round(tn / (tn+fp),2)\n",
    "    sensitivityRF = round(tp / (tp + fn),2)\n",
    "    precisionRF= round(metrics.precision_score(y, y_pred),2)\n",
    "    f1RF = round(metrics.f1_score(y, y_pred),2)\n",
    "    \n",
    "    \n",
    "    print(\"2.6 XGBoost\\n\")\n",
    "    print(\"a) Suggested parameterization: learning_rate=0.08 with SMOTE and KBest feature selection\")\n",
    "\n",
    "\n",
    "    model = xgb.XGBClassifier(random_state=1,learning_rate=0.08)\n",
    "    scoresNB = []\n",
    "    y_predict_total =[]\n",
    "    y_test_total =[]\n",
    "    tn, fp, fn, tp = 0,0,0,0\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    tn, fp, fn, tp = confusion_matrix(y, y_pred).ravel()\n",
    "    \n",
    "    acuXGBoost = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    #acuNB = round((tp+tn)/(tp+tn+fp+fn),2)\n",
    "    specificityXGBoost = round(tn / (tn+fp),2)\n",
    "    sensitivityXGBoost = round(tp / (tp + fn),2)\n",
    "    precisionXGBoost= round(metrics.precision_score(y, y_pred),2)\n",
    "    f1XGBoost = round(metrics.f1_score(y, y_pred),2)\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "    print(\"b) Confusion matrix: \")\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(conf_mat)\n",
    "\n",
    "          \n",
    "    print(\"3. Comparative performance: NB | kNN | DT | RF | XGBoost\")\n",
    "\n",
    "    print(\"3.1 Accuracy: \")\n",
    "    \n",
    "    print(str(acuNB)+\" | \"+ str(acukNN) +\" | \"+ str(acuDT)+\" | \"+ str(acuRF)+\" | \"+ str(acuXGBoost))\n",
    "\n",
    "    # 0.76 | 0.81 | 0.56 | 0.90 3.2 \n",
    "\n",
    "    print(\"3.2 Sensitivity: \")\n",
    "    #  0.76 | 0.81 | 0.56 | 0.90 3.2 \n",
    "    print(str(specificityNB)+\" | \"+ str(specificitykNN) +\" | \"+\n",
    "          str(specificityDT)+\" | \"+ str(specificityRF)+\" | \"+ str(specificityXGBoost))\n",
    "\n",
    "    print(\"3.3 Sensitivity: \")\n",
    "    #  0.76 | 0.81 | 0.56 | 0.90 3.2 \n",
    "    \n",
    "    print(str(sensitivityNB)+\" | \"+ str(sensitivitykNN) +\" | \"+\n",
    "          str(sensitivityDT)+\" | \"+ str(sensitivityRF)+\" | \"+ str(sensitivityXGBoost))\n",
    "    \n",
    "    print(\"3.5 Precision: \")\n",
    "    #  0.76 | 0.81 | 0.56 | 0.90 3.2 \n",
    "    \n",
    "    print(str(precisionNB)+\" | \"+ str(precisionkNN) +\" | \"+\n",
    "          str(precisionDT)+\" | \"+ str(precisionRF)+\" | \"+ str(precisionXGBoost))\n",
    "    \n",
    "    print(\"3.6 F1-Score: \")\n",
    "    #  0.76 | 0.81 | 0.56 | 0.90 3.2 \n",
    "    \n",
    "    print(str(f1NB)+\" | \"+ str(f1kNN) +\" | \"+\n",
    "          str(f1DT)+\" | \"+ str(f1RF)+\" | \"+ str(f1XGBoost))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def classification_ct_report(data):\n",
    "    y = data.pop('Cover_Type').values\n",
    "    df2 = pd.DataFrame(y, columns=['Cover_Type'])\n",
    "    transf = Normalizer().fit(data)\n",
    "    norm_data = pd.DataFrame(transf.transform(data, copy=True), columns= data.columns)\n",
    "    norm_data = pd.concat([norm_data, df2], axis=1)\n",
    "    unbal = norm_data\n",
    "    \n",
    "    X = unbal.drop(columns=['Cover_Type'])\n",
    "    y = unbal['Cover_Type'].values\n",
    "    \n",
    "    sampler = RandomUnderSampler(random_state=12)\n",
    "    X_res, y_res = sampler.fit_resample(X, y)\n",
    "    \n",
    "    \n",
    "    data = pd.DataFrame(X_res, columns=unbal.columns[:-1])\n",
    "    data.describe(include='all')\n",
    "    df2 = pd.DataFrame(y_res, columns=['Cover_Type'])\n",
    "    data = pd.concat([data, df2], axis=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"\\n 2. Classifiers:\")\n",
    "\n",
    "    labels = pd.unique(y)\n",
    "\n",
    "   \n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=None, stratify=y)\n",
    "    \n",
    "    cv = KFold(n_splits=10, random_state=42, shuffle=False)\n",
    "    \n",
    "    y = data.pop('Cover_Type').values\n",
    "    X = data.values\n",
    "\n",
    "    print(\"\\n 2.1 NB\")\n",
    "    print(\"a) Suggested parameterization: MultinomialNB with Undersampling\")\n",
    "\n",
    "    model = MultinomialNB()\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    from sklearn.metrics import multilabel_confusion_matrix\n",
    "    \n",
    "    acuNB = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    \n",
    "    print(\"b) Confusion matrix: \")\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(conf_mat)\n",
    "    \n",
    "    \n",
    "    print(\"\\n 2.2 kNN\")\n",
    "    print(\"a) Suggested parameterization: n_neighbors=1, metric='manhattan' with Undersampling\")\n",
    "\n",
    "    model = KNeighborsClassifier(n_neighbors=1, metric='manhattan')\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    acukNN = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    \n",
    "    print(\"b) Confusion matrix: \")\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(conf_mat)\n",
    "    \n",
    " \n",
    "\n",
    "    print(\"\\n 2.2 DT\")\n",
    "    print(\"a) Suggested parameterization: max_depth=13, min_samples_split=0.0002, min_samples_leaf=0.00005, criterion='entropy' with Undersampling\")\n",
    "\n",
    "    model = DecisionTreeClassifier(max_depth=13, min_samples_split=0.0002, min_samples_leaf=0.00005, criterion=\"entropy\", random_state=1)\n",
    "\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    acuDT = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    \n",
    "    print(\"b) Confusion matrix: \")\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(conf_mat)\n",
    "    \n",
    " \n",
    "\n",
    "    print(\"\\n 2.2 RF\")\n",
    "    print(\"a) Suggested parameterization: n_estimators=100, max_depth=25, max_features='log2' with Undersampling\")\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=25, max_features='log2')\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    acuRF = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    \n",
    "    print(\"b) Confusion matrix: \")\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(conf_mat)\n",
    "    \n",
    "    \n",
    "    print(\"\\n 2.2 XGBoost\")\n",
    "    print(\"a) Suggested parameterization: learning_rate=0.1 with Undersampling\")\n",
    "\n",
    "    model = xgb.XGBClassifier(random_state=1,learning_rate=0.1)\n",
    "    \n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "    X = pd.DataFrame(X)\n",
    "    \n",
    "    y_pred = cross_val_predict(model, X, y, cv=10)\n",
    "    \n",
    "    acuXGBoost = round(metrics.accuracy_score(y, y_pred),2)\n",
    "    \n",
    "    print(\"b) Confusion matrix: \")\n",
    "\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print(conf_mat)\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"3. Comparative performance: NB | kNN | DT | RF | XGBoost \")\n",
    "\n",
    "    print(\"3.1 Accuracy: \")\n",
    "    \n",
    "    print(str(acuNB)+\" | \"+ str(acukNN) +\" | \"+ str(acuDT)+\" | \"+ str(acuRF)+\" | \"+ str(acuXGBoost))\n",
    "\n",
    "    # 0.76 | 0.81 | 0.56 | 0.90 3.2 \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def report(source, dataframe, task):\n",
    "    task = task.strip()\n",
    "    if task == \"preprocessing\":\n",
    "        if source == \"PD\":\n",
    "            return preprocessing_pd_report(dataframe)\n",
    "        if source == \"CT\":\n",
    "            return preprocessing_ct_report(dataframe)\n",
    "    \n",
    "    if task == \"unsupervised\":\n",
    "        if source == \"PD\":\n",
    "            return unsupervised_pd_report(dataframe)\n",
    "        if source == \"CT\":\n",
    "            return unsupervised_ct_report(dataframe)\n",
    "    \n",
    "    if task == \"classification\":\n",
    "        if source == \"PD\":\n",
    "            return classification_pd_report(dataframe)\n",
    "        if source == \"CT\":\n",
    "            return classification_ct_report(dataframe)\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    return \"Not yet available.\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    '''A: read arguments'''\n",
    "    #args = sys.stdin.readline().rstrip('\\n').split(' ')\n",
    "    #n, source, task = int(args[0]), args[1], args[2]\n",
    "    \n",
    "    '''B: read dataset'''\n",
    "    #data, header = [], sys.stdin.readline().rstrip('\\r\\n').split(',')\n",
    "    \n",
    "    #for i in range(n-1):\n",
    "    #    data.append(sys.stdin.readline().rstrip('\\r\\n').split(','))\n",
    "    ## Tudo forcado a ser float64 pois eram objetos\n",
    "    #dataframe = pd.DataFrame(data, columns=header, dtype=float)\n",
    "    \n",
    "    #dataframe = pd.read_csv('pd_speech_features.csv', sep=',')\n",
    "    dataframe = pd.read_csv('convAfterUndersampling.csv', sep=',')\n",
    "    task = \"classification\"\n",
    "    source = \"CT\"\n",
    "    \n",
    "    \n",
    "    '''C: output results'''\n",
    "    print(report(source, dataframe, task))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
